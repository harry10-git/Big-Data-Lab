{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Set up the environment variables for PySpark to run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebf5876766db4773"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T14:07:27.428631Z",
     "start_time": "2024-04-06T14:07:27.249797Z"
    }
   },
   "id": "f8a1c3ced044a93e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9122f141ae987a47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a SparkSession object by allocating 16GB memory to the Spark Driver\n",
    "Either get the exisiting SparkSession or create one with application names as Kmeans"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fb7d9370b07a603"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config('spark.driver.memory', '16g').appName('k_means').getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc90792dd21c825a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KMeans\n",
    "KMeans is an unsupervised machine learning algorithm used for clustering data points into groups or clusters based on similarity. \n",
    "It aims to minimize the within-cluster variance, also known as inertia or distortion. It's computationally efficient and works well with large datasets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55c10dd1528db670"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the csv file from the data directory.\n",
    "Infer the Schema of the columns automatically , but also mention that no header row is present in csv file.\n",
    "\n",
    "Define a list of all column names present in the list and apply these columns to the dataframe we have"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29378b6e6d030a30"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_without_header = spark.read.option('inferSchema', True).option('header', False).csv('data/kddcup.data_10_percent_corrected')\n",
    "\n",
    "column_names = [ \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "\"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "\"hot\"\n",
    ", \"num_failed_logins\", \"logged_in\", \"num_compromised\"\n",
    "\"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "\"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "\"is_host_login\", \"is-guest_login\", \"count\", \"srv_count\",\n",
    "\"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "\"same_srv_rate\", \"diff_srv_rate\", \"srv_diff _host_rate\",\n",
    "\"dst_host_count\", \"dst_host_srv_count\"\n",
    "\"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "\"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "\"dst_host_serror_rate\", \"dst_host_srv_serror_rate\"\n",
    "\"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "\"label\"]\n",
    "data = data_without_header. toDF (*column_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2d7fd42c83ee908"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To refer to dataframe by column names, import the col function\n",
    "Calculate the frequency of each unique label in the 'label' column of the dataframe and display the top 25 results in descending order"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2be7de8a6f66ea45"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col \n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show (25)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38a9305d3edc146d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import VectorAssembler to assemble feature columns into vector column\n",
    "Import Kmeans class to implement the Kmeans algorithm\n",
    "Import Pipleline class to chain multiple transformations\n",
    "\n",
    "Create a new dataframe numeric_only by dropping the mentioned columns. Cache this dataframe for faster access\n",
    "\n",
    "Create a vector assembler specifying input columns as all except the last one"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c317306723bca85"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml. clustering import KMeans, KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\"). cache()\n",
    "assembler = VectorAssembler().setInputCols (numeric_only. columns [: -1]) . \\\n",
    "    setOutputCol (\"'featureVector\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2134ff2f6b67c5c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a Kmeans object and set ther feature column name as 'featureVector' and prediction column name as 'cluster'\n",
    "Then create a pipeline object setting its stages to assembler nad Kmeans\n",
    "Fit the earlier created numeric_only dataframe to this pipeline model\n",
    "Then retrive the Kmeans model from the fitter pipeline model\n",
    "\n",
    "Import the pprint function for pretty printing the cluster centers. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff49712424a6a199"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"'featureVector\")\n",
    "pipeline = Pipeline().setStages([assembler, kmeans] )\n",
    "pipeline_model = pipeline.fit(numeric_only)\n",
    "kmeans_model = pipeline_model.stages[1]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint (kmeans_model. clusterCenters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75bf4e5731e3dbd0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply the trained Kmeans model to dataset\n",
    "Calculates the count of each combination of cluster and label in the dataset, ordered by cluster and count"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77e0b6cef1078600"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with_cluster = pipeline_model.transform(numeric_only)\n",
    "\n",
    "with_cluster.select(\"cluster\", \"label\").groupBy (\"cluster\", \"label\").count().orderBy (col(\"cluster\"), col (\"count\").desc()).show (25)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7aacf50017fd717f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a function clustering_score that calculates the training cost of KMeans clustering.\n",
    "Iterate over the value of 20 to 100 in steps of 20.\n",
    "Create a VectorAssembler specifying all columns except last from only numeric inputs dataframe. \n",
    "Create a KMeans object with random seed and k clusters\n",
    "Create a pipeline with assembler and Kmeans as stages\n",
    "\n",
    "Find the training cost using ther sum of squared distances of points method.\n",
    "Finally print the training_cost"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "492377ca0ab260be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from random import randint\n",
    "\n",
    "def clustering_score (input_data, k):\n",
    "    input_numeric_only = input_data.drop (\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only. columns[: -1]).setOutputCol (\"'featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint (100,100000)).setK(k).setPredictionCol (\"cluster\").setFeaturesCol (\"featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans] )\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model. stages [-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost    \n",
    "\n",
    "for k in list(range(20,100,20)):\n",
    "    print(clustering_score(numeric_only, k))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf0638a09e5d14e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare the input dataset by only keeping numeric columns\n",
    "Perform similar steps as previous cell , only with once additional k val"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31325debcc94f3b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clustering_score_1(input_data, k):\n",
    "    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "    \n",
    "    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"'featureVector\")\n",
    "    kmeans = KMeans().setSeed(randint(100, 100000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"'featureVector\")\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    kmeans_model = pipeline_model.stages[-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost    \n",
    "    return training_cost\n",
    "\n",
    "for k in list(range(20,101,20)):\n",
    "    print(k , clustering_score_1(numeric_only, k))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d94065ec10a037d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use StandardScaler to standardize features\n",
    "Set the Kmeans algorithm with specified number of features and parameters\n",
    "Create a pipeline with VectorAssembler, StandardScaler, and KMeans estimator\n",
    "Extract the KMeans model from the fitted pipeline model\n",
    "Calculate training cost of the KMeans model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b164a27bfd48f041"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml. feature import StandardScaler\n",
    "def clustering_score_2(input_data, k):\n",
    "    input_numeric_only = input_data.drop (\"protocol_type\", \"service\", \"flag\")\n",
    "    assembler = VectorAssembler.\\\n",
    "    setInputCols(input_numeric_only.columns [: -1]) .\\\n",
    "    setOutputCol (\"'featureVector\")\n",
    "    scaler = StandardScaler().setInputCol (\"featureVector\"). \\\n",
    "    setOutputCol (\"scaledFeatureVector\"). \\\n",
    "    setWithStd (True). setWithMean (False)\n",
    "    kmeans = KMeans().setSeed(randint (100,100000)) .\\\n",
    "    setK(k) .setMaxIter (40) .\\\n",
    "    setTol (1.0e-5) .setPredictionCol(\"cluster\"). \\\n",
    "    setFeaturesCol (\"scaledFeatureVector\")\n",
    "    pipeline =Pipeline().setStages([assembler, scaler, kmeans])\n",
    "    pipeline_model = pipeline.fit(input_numeric_only)\n",
    "    means_model = pipeline_model.stages [-1]\n",
    "    training_cost = kmeans_model.summary.trainingCost\n",
    "    return training_cost\n",
    "\n",
    "for k in list (range (60, 271, 30)) :\n",
    "    print(k, clustering_score_2 (numeric_only, k))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf435c9a85ebc1a8"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d486fbdc1020e8a8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def entropy (counts) :\n",
    "    values = [c for c in counts if (c > 0)]\n",
    "    n = sum (values)\n",
    "    p = [v/n for v in values]\n",
    "    return sum([-1*(p_v) * log(p_v) for p_v in p])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0164c5d527899bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform the data using the fitted pipeline model and select the cluster and label columns\n",
    "Group the data by cluster and label, and count the occurrences of each combination\n",
    "define a window partitioned by cluster for later use\n",
    "Calculate the probability of each label within each cluster\n",
    "Calculate the entropy and weighted cluster for each cluster \n",
    "Calculate the sum of weighted cluster entropy across all clusters\n",
    "Calculate the weighted average cluster entropy by dividing by the total number of data points"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e92162c157ad8937"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import  Window\n",
    "\n",
    "cluster_label = pipeline_model.transform (data).\\\n",
    "                select (\"cluster\", \"label\")\n",
    "df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
    "w = Window. partitionBy (\"cluster\")\n",
    "p_col = df ['count'] / fun. sum (df ['count ']) . over (w)\n",
    "with_p_col = df.withColumn (\"p_col\", p_col)\n",
    "result = with_p_col.groupBy('cluster').agg((-fun.sum(col('p_col')*fun.log2(col('p_col')))).\\\n",
    "                                           alias('entropy'),\n",
    "                                           fun.sum(col('count')).alias('cluster_size'))\n",
    "result = result.withColumn ( 'weightedClusterEntropy',fun.col('entropy') * fun.col('cluster_size'))\n",
    "weighted_cluster_entropy_avg = result.agg (fun. sum(col ( 'weightedClusterEntropy'))).\\\n",
    "    collect ()\n",
    "weighted_cluster_entropy_avg [0] [0]/data. count ()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da07b124e42649ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entity Resolution\n",
    "\n",
    "Entity resolution, also known as record linkage or deduplication, is a data integration process that identifies \n",
    "and links records that refer to the same real-world entity across diverse data sources. The goal is to \n",
    "reconcile and merge information about entities, such as individuals or businesses, even when they are \n",
    "represented inconsistently or incompletely in different datasets. Entity resolution involves comparing and \n",
    "analyzing attributes like names, addresses, and other identifying information to determine the likelihood \n",
    "of a match. This process is crucial in various domains, including customer relationship management, \n",
    "healthcare, finance, and law enforcement, where accurate and consolidated data is essential. Advanced \n",
    "techniques, such as probabilistic matching and machine learning algorithms, are often employed to \n",
    "enhance the accuracy and efficiency of entity resolution in handling large and complex datasets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39a42259d8a3f81b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading and Preparation\n",
    "\n",
    "\n",
    "We start by creating a SparkSession, the entry point to Spark functionality. Upon loading the data, the schema of the data is inferred and printed. After that, we cache the parsed DataFrame to optimize future operations. \n",
    "\n",
    "We display the first five rows of the DataFrame and perform a grouping operation on the 'is_match' column, counting the occurrences of each unique value and displaying the results in descending order. Finally, we create a temporary view named 'linkage' from the parsed DataFrame, which can be used to execute SQL queries on the DataFrame using Spark SQL.\n",
    "\n",
    "We're performing the same operation as the previous PySpark DataFrame operation but using SQL syntax through the SparkSession. This query groups the DataFrame by the 'is_match' column, counts the occurrences, and orders the results by count in descending order. Finally, it displays the results with the count column aliased as 'cnt'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5b0e0ef9198c122"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "prev = spark.read.option(\"recursiveFileLookup\",\"true\").csv(\"data/linkage/donation/block_1.csv\")\n",
    "\n",
    "prev.show(3)\n",
    "\n",
    "parsed = spark.read.option(\"header\",\"true\").option(\"nullValue\",\"?\").option(\"inferSchema\",\"true\").csv(\"data//linkage//donation//block_1.csv\")\n",
    "parsed.printSchema()\n",
    "parsed.count()\n",
    "parsed.cache()\n",
    "\n",
    "parsed.show(5)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "parsed.createOrReplaceTempView(\"linkage\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT is_match,count(*) cnt\n",
    "    FROM linkage\n",
    "    GROUP BY is_match\n",
    "    ORDER BY cnt DESC\"\"\").show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "532796baba2c7606"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary Statistics\n",
    "\n",
    "The code is conducting statistical summaries of specific columns within the DataFrame 'parsed'. \n",
    "\n",
    "1. `summary = parsed.describe()`: This line generates summary statistics for all numerical columns in the DataFrame 'parsed'. The resulting DataFrame 'summary' contains statistical summaries such as count, mean, standard deviation, min, and max.\n",
    "\n",
    "2. `summary.select(\"summary\",\"cmp_fname_c1\",\"cmp_fname_c2\").show()`: This line selects and displays summary statistics for the columns 'cmp_fname_c1' and 'cmp_fname_c2' from the DataFrame 'summary'. The 'summary' column provides information on what type of statistical summary it is (e.g., count, mean, stddev, min, and max).\n",
    "\n",
    "3. `matches = parsed.where(\"is_match = true\")`: This line filters the DataFrame 'parsed' to include only rows where the column 'is_match' is equal to 'true', thus selecting only the rows where matches occurred.\n",
    "\n",
    "4. `match_summary = matches.describe()`: This line generates summary statistics for all numerical columns in the DataFrame 'matches', which now contains only the rows where matches occurred.\n",
    "\n",
    "5. `misses = parsed.filter(col('is_match') == False)`: This line filters the DataFrame 'parsed' to include only rows where the column 'is_match' is equal to 'false', thus selecting only the rows where no matches occurred.\n",
    "\n",
    "6. `miss_summary = misses.describe()`: This line generates summary statistics for all numerical columns in the DataFrame 'misses', which now contains only the rows where no matches occurred.\n",
    "\n",
    "Overall, these operations help in understanding the distribution and characteristics of data related to matches and non-matches separately."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f03869a9438f80e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summary = parsed.describe()\n",
    "summary.select(\"summary\",\"cmp_fname_c1\",\"cmp_fname_c2\").show()\n",
    "\n",
    "matches = parsed.where(\"is_match = true\")\n",
    "match_summary = matches.describe()\n",
    "\n",
    "misses = parsed.filter(col('is_match') == False)\n",
    "miss_summary = misses.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72e9d4ed28a24b38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pivoting and Reshaping DataFrames\n",
    "\n",
    "We begin by converting the PySpark DataFrame `summary` into a Pandas DataFrame `summary_p` to facilitate data manipulation using Pandas functions. Then, we transpose the DataFrame, set the index to 'summary', rename columns, and convert it back to a Spark DataFrame `summaryT`.\n",
    "\n",
    "Next, we cast the metric columns to DoubleType to ensure numerical consistency and print the schema of the resulting DataFrame.\n",
    "\n",
    "We define a function `pivot_summary(desc)` that performs similar operations as above but takes a DataFrame `desc` as input. It returns the transposed and type-casted Spark DataFrame `descT`.\n",
    "\n",
    "We apply the `pivot_summary` function to both `match_summary` and `miss_summary` DataFrames to obtain `match_summaryT` and `miss_summaryT`, respectively.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4898f4f59152830"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "summary_p = summary.toPandas()\n",
    "summary_p.head()\n",
    "print(summary_p.shape)\n",
    "summary_p = summary_p.set_index('summary').transpose().reset_index()\n",
    "summary_p = summary_p.rename(columns={'index':'field'})\n",
    "summary_p = summary_p.rename_axis(None, axis=1)\n",
    "print(summary_p.shape)\n",
    "\n",
    "summaryT = spark.createDataFrame(summary_p)\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "for c in summaryT.columns:\n",
    "    if c == 'field':\n",
    "        continue\n",
    "    summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))\n",
    "\n",
    "summaryT.printSchema()\n",
    "\n",
    "\n",
    "def pivot_summary(desc):\n",
    "    # convert to pandas dataframe\n",
    "    desc_p = desc.toPandas()\n",
    "    # transpose\n",
    "    desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "    desc_p = desc_p.rename(columns={'index':'field'})\n",
    "    desc_p = desc_p.rename_axis(None, axis=1)\n",
    "    # convert to Spark dataframe\n",
    "    descT = spark.createDataFrame(desc_p)\n",
    "    # convert metric columns to double from string\n",
    "    for c in descT.columns:\n",
    "        if c == 'field':\n",
    "            continue\n",
    "        else:\n",
    "            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "    return descT\n",
    "\n",
    "match_summaryT = pivot_summary(match_summary)\n",
    "miss_summaryT = pivot_summary(miss_summary)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a97e21c47841d593"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Joining Dataframes and Selecting Good Features\n",
    "\n",
    "We create temporary views for `match_summaryT` and `miss_summaryT` named \"match_desc\" and \"miss_desc\", respectively.\n",
    "\n",
    "We execute a SQL query to join the two DataFrames on the `field` column, calculate the total count, and compute the difference in mean values between matches and misses for each field. The results are ordered by delta and total count in descending order.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77046aa3a2e25e9b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "match_summaryT.createOrReplaceTempView(\"match_desc\")\n",
    "miss_summaryT.createOrReplaceTempView(\"miss_desc\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT a.field, a.count + b.count total, a.mean - b.mean delta\n",
    "    FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field\n",
    "    WHERE a.field NOT IN (\"id_1\", \"id_2\")\n",
    "    ORDER BY delta DESC, total DESC\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46e4cedbe6ac2b99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scoring and Model Evaluation \n",
    "\n",
    "We define a list of `good_features` and an expression `sum_expression` to calculate the sum of these features.\n",
    "\n",
    "We start by filling null values with 0 in the columns specified in `good_features` in the DataFrame `parsed`. Then, we calculate a new column `score` based on the sum of values in the columns specified in `good_features`, and select only the `score` and `is_match` columns from the DataFrame `parsed`.\n",
    "\n",
    "Next, we define a function named `crossTabs` that takes a DataFrame `scored` and a threshold `t` as input and returns a DataFrame.\n",
    "\n",
    "Then, we compute cross-tabulations by grouping the DataFrame `scored` based on whether the `score` is above or equal to the threshold `t`, pivot the `is_match` column, and count the occurrences of each combination.\n",
    "\n",
    "We calculate cross-tabulations using the function `crossTabs` for the threshold `4.0` and store the result in `cm1`, and similarly for the threshold `2.0`, storing the result in `cm2`.\n",
    "\n",
    "Finally, we display cross-tabulations using the function `crossTabs` for the threshold `4.0`. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b811347867b6982d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "sum_expression = \" + \".join(good_features)\n",
    "\n",
    "scored = parsed.fillna(0,subset=good_features).withColumn('score',expr(sum_expression)).select('score','is_match')\n",
    "scored.show()\n",
    "\n",
    "def crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame:\n",
    "    return scored.selectExpr(f\"score>={t} as above\",\"is_match\").groupBy(\"above\").pivot(\"is_match\",(\"true\",\"false\")).count()\n",
    "\n",
    "cm1 = crossTabs(scored,4.0)\n",
    "cm2 = crossTabs(scored,2.0)\n",
    "crossTabs(scored,4.0).show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "902348cb3eaec756"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dfafd83b2fbca442"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ccb0f1e19b1d371"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Tree\n",
    "Decision trees are a fundamental and widely used machine learning algorithm for both classification and regression tasks. They are intuitive, easy to understand, and can handle both numerical and categorical data.\n",
    "\n",
    "The construction of a decision tree involves recursively splitting the dataset into subsets based on the value of the most informative feature at each step. This process continues until all instances in a node belong to the same class (in the case of classification) or until a stopping criterion is met (in the case of regression)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eafad95add6461af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config('spark.driver.memory', '16g').appName('k_means').getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74864e6dd1a17b7e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the dataset.\n",
    "The dataset does not have a header row"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74179b11c8f3cbc2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_without_header = spark.read.option('inferSchema', True).option('header', False)\\\n",
    "    .csv('data/covtype.csv')\n",
    "\n",
    "data_without_header.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d86634f29598890d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define column names in colnames\n",
    "Then Cast the Cover_Type column to Double"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a01ea303ab8b06c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define column names as a tuple\n",
    "colnames = (\"Elevation\", \"Aspect\", \"Slope\",\n",
    "            \"Horizontal_Distance_To_Hydrology\",\n",
    "            \"Vertical_Distance_To_Hydrology\",\n",
    "            \"Horizontal_Distance_To_Roadways\",\n",
    "            \"Hillshade_9am\", \"Hillshade_Noon\",\n",
    "            \"Hillshade_3pm\",\n",
    "            \"Horizontal_Distance_To_Fire_Points\") + \\\n",
    "           [f\"Wilderness_Area_{i}\" for i in range(4)] + \\\n",
    "           [f\"Soil_Type_{i}\" for i in range(40)] + \\\n",
    "           [\"Cover_Type\"]\n",
    "\n",
    "# Create DataFrame with specified column names\n",
    "data = data_without_header.toDF(*colnames).withColumn(\"Cover_Type\", col(\"Cover_Type\").cast(DoubleType()))\n",
    "data.head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "211f0df4647b7677"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define column names in colnames\n",
    "Then Cast the Cover_Type column to Double"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cdbd3c021555b4f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define column names as a tuple\n",
    "colnames = (\"Elevation\", \"Aspect\", \"Slope\",\n",
    "            \"Horizontal_Distance_To_Hydrology\",\n",
    "            \"Vertical_Distance_To_Hydrology\",\n",
    "            \"Horizontal_Distance_To_Roadways\",\n",
    "            \"Hillshade_9am\", \"Hillshade_Noon\",\n",
    "            \"Hillshade_3pm\",\n",
    "            \"Horizontal_Distance_To_Fire_Points\") + \\\n",
    "           [f\"Wilderness_Area_{i}\" for i in range(4)] + \\\n",
    "           [f\"Soil_Type_{i}\" for i in range(40)] + \\\n",
    "           [\"Cover_Type\"]\n",
    "\n",
    "# Create DataFrame with specified column names\n",
    "data = data_without_header.toDF(*colnames).withColumn(\"Cover_Type\", col(\"Cover_Type\").cast(DoubleType()))\n",
    "data.head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d64c61fbd492cdc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the data to train and test in the ratio 0.9 to 0.1\n",
    "Cache both the train and test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b882930306fe3b05"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "(train_data, test_data) = data.randomSplit([0.9, 0.1])\n",
    "train_data.cache()\n",
    "test_data.cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfce3cc4a1802c0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define input columns as all columns except the last one\n",
    "Create a VectorAssembler instance specifying input and output columns\n",
    "Transform the train_data DataFrame using the VectorAssembler\n",
    "Display the assembled feature vector column"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3d83abc9f7dcf15"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml. feature import VectorAssembler\n",
    "input_cols = colnames [: -1]\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols,outputCol=\"featureVector\")\n",
    "assembled_train_data = vector_assembler. transform (train_data)\n",
    "assembled_train_data.select(\"featureVector\").show(truncate = False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b9975df29e278c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a DecisionTreeClassifier instance\n",
    "Fit the classifier to the assembled training data\n",
    "Print the decision tree model's debug string"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac57388ebf55763d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml. classification import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier (seed = 1234, labelCol=\"Cover_Type\",featuresCol=\"featureVector\" predictionCol=\"prediction\")\n",
    "model = classifier. fit (assembled_train_data)\n",
    "print (model. toDebugString)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e70da99c436d1ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert feature importances to a Pandas DataFrame\n",
    "Then sort feature importances by importance in descending order\n",
    "Finally print the Dataframe"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59a656c051f7b96f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(model.featureImportances.toArray(),\n",
    "             index=input_cols, columns=['importances']).\\\n",
    "            sort_values(by='importance', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a5d4c03dfea319b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply the trained model to the assembled training data to make predictions\n",
    "Then Select and display Cover_Type , prediction and probability"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89ce9b40a06e6e9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "predictions = model.transform(assembled_train_data)\n",
    "predictions.select('Cover_Type', 'prediction', 'probability').show(10, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72b583626cf20ed2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a MulticlassClassificationEvaluator instance with specified parameters\n",
    "Set the metric name to evaluate the accuracy\n",
    "Set the metric name to evaluate the F1 score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d219ff8b59700c3a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator (labelCol=\"Cover_Type\",predictioncol=\"prediction\")\n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "evaluator.setMetricName(\"f1\").evaluate (predictions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5966c9356ff88354"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Group predictions by actual cover type and pivot on the predicted cover types\n",
    "Then display the confusion matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f57b8fc6f6d355de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "confusion_matrix = predictions.groupBy(\"Cover_Type\"). \\\n",
    "    pivot (\"prediction\", range (1,8)).count().\\ \n",
    "    na.fill(0.0).orderBy (\"Cover _Type\")\n",
    "\n",
    "confusion_matrix.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d20ba95641475ba8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a class probabilites function and count total number of instances in data\n",
    "Group data by \"Cover_Type\", count instances, and calculate proportions\n",
    "Return the probabilities as a list of tuples\n",
    "Calculate class probabilities for train_data and test_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50f1905ad1fde721"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "def class_probabilities (data):\n",
    "    total = data.count\n",
    "    return data.groupBy(\"Cover_Type\").count().orderBy(\"'Cover_Type\").select (col (\"count\"). cast (DoubleType())) .\\\n",
    "    withColumn (\"count _proportion\", col(\"count\") /total).\\\n",
    "    select (\"count_proportion\"). collect ()\n",
    "\n",
    "train_prior_probabilites = class_probabilities(train_data)\n",
    "test_prior_probabilites = class_probabilities(test_data)\n",
    "test_prior_probabilites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ce4845eb9099957"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate the weighted average of prior probabilities by multiplying the prior probabilities of each class in the training data by the corresponding prior probabilities of each class in the testing data and summing up the results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9da30e504dce71a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_prior_probabilites = [p[0] for p in train_prior_probabilites]\n",
    "test_prior_probabilites = [p[0] for p in test_prior_probabilites]\n",
    "sum([train_p * cv_p for train_p,cv_p in zip(train_prior_probabilites, test_prior_probabilites)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "151be7f300af4389"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a VectorAssembler to assemble input features into a single vector column\n",
    "Define a DecisionTreeClassifier with specified parameters\n",
    "Create a Pipeline with the VectorAssembler and DecisionTreeClassifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6519a61e81acf3e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "\n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
    "                                    featuresCol=\"featureVector\", predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[assembler, classifier])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd71eddde343c71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define parameter grid for hyperparameter tuning\n",
    "Create a MulticlassClassificationEvaluator for evaluation named multiclassEval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5fca12ecd16ef32"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(classifier.impurity, [\"gini\", \"entropy\"])\n",
    "             .addGrid(classifier.maxDepth, [1, 20])\n",
    "             .addGrid(classifier.maxBins, [40, 300])\n",
    "             .addGrid(classifier.minInfoGain, [0.0, 0.05])\n",
    "             .build())\n",
    "\n",
    "multiclassEval = (MulticlassClassificationEvaluator()\n",
    "                  .setLabelCol(\"Cover_Type\")\n",
    "                  .setPredictionCol(\"prediction\")\n",
    "                  .setMetricName(\"accuracy\"))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6b93ced70deb49a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a TrainValidationSplit instance for hyperparameter tuning\n",
    "Fit the TrainValidationSplit on the training data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea38f531d877fe97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "validator = TrainValidationSplit(seed=1234,\n",
    "                                 estimator=pipeline,\n",
    "                                 evaluator=multiclassEval,\n",
    "                                 estimatorParamMaps=paramGrid,\n",
    "                                 trainRatio=0.9)\n",
    "validator_model = validator.fit(train_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fca9a6d581b194d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract the best model from the TrainValidationSplit model\n",
    "Extract and pprint the parameter map of the classifier stage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dff356af7c597002"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "best_model = validator_model.bestModel\n",
    "pprint(best_model.stages[1].extractParamMap())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18db3b84f1291cbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit the TrainValidationSplit on the training data again\n",
    "Get the validation metrics and estimator param maps from the fitted validator model\n",
    "Combine metrics and params into a list of tuples\n",
    "Sort the list of tuples based on validation metrics in descending orderDisplay the sorted list of tuples containing validation metrics and corresponding parameter maps\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "499f11180deacd58"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "validator_model = validator.fit(train_data)\n",
    "\n",
    "metrics = validator_model.validationMetrics\n",
    "params = validator_model.getEstimatorParamMaps()\n",
    "metrics_and_params = list(zip(metrics, params))\n",
    "metrics_and_params.sort(key=lambda x: x[0], reverse=True)\n",
    "metrics_and_params"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce46797f4ec4f394"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sort the metrics list in reverse order and print the highest validation metric achieved"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece06feacfbd872a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metrics.sort(reverse= True)\n",
    "print(metrics[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "806639ab9830ac7e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate the best model on the test data and print the accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a8bba2ba2e0f119"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multiclassEval.evaluate(best_model.transform(test_data))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89e81ff4392d07a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recommendation System\n",
    "\n",
    "A recommendation system is a software application that suggests items or content to users based on their preferences, behaviors, and historical interactions. It leverages algorithms to analyze user data and identify patterns, aiming to provide personalized and relevant recommendations. There are two primary types of recommendation systems: collaborative filtering, which recommends items based on the preferences of users with similar tastes, and content-based filtering, which suggests items similar to those the user has previously liked. Hybrid approaches combine these methods for more robust and \n",
    "accurate suggestions. Recommendation systems are widely used in e-commerce platforms, streaming services, social media, and other online applications to enhance user experience, engagement, and satisfaction by delivering tailored content or product suggestions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2323ec89bb553f71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading and Preparation\n",
    "\n",
    "The first step in building a model is to understand the data that is available, and parse or transform it into forms that are useful for analysis in Spark.\n",
    "\n",
    "Spark MLlib’s ALS implementation does not strictly require numeric IDs for users and items, but is more efficient when the IDs are in fact representable as 32-bit integers. It’s advantageous to use Int to represent IDs, but this would mean that the IDs can’t exceed Int.MaxValue, or 2147483647. \n",
    "\n",
    "Code explanation:\\\n",
    "Each line of the file contains a user ID, an product ID, and a score. The json file is read and converted to a Spark DataFrame"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "352ffe1056221c04"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "ratings = spark.read.json(\"movies 1.json\").select(\"user_id\",\"product_id\",\"score\").cache()\n",
    "ratings = ratings.head(10000)\n",
    "ratings = spark.createDataFrame(ratings)\n",
    "\n",
    "ratings.show(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2116c3f6f8b64ec5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting and Training the Pre-processed data\n",
    "\n",
    "From pyspark.ml we import the following libraries: \n",
    "ALS: This module implements the Alternating Least Squares (ALS) algorithm, which is commonly used for collaborative filtering. \n",
    "RegressionEvaluator: It evaluates the performance of regression models.\n",
    "StringIndexer: Used to convert categorical variables into numerical indices.\n",
    "Pipeline: Allows you to chain multiple transformers and estimators into a single pipeline.\n",
    "\n",
    "The indexers will convert string columns to numerical indices. A Pipeline object is created to execute all the stages (indexers) in order. The fit() method of the pipeline is called with the original ratings DataFrame to fit the indexers, and then the transform() method is called to transform the data. This results in a new DataFrame ratings_indexed where the \"user_id\" and \"product_id\" columns are replaced with their respective numerical indices.\n",
    "\n",
    "The randomSplit() method is used to split the indexed ratings data into training and validation sets in an 8:2 ratio. An ALS object is created with specified parameters. A RegressionEvaluator object is created to evaluate the model's performance. It will calculate the RMSE (Root Mean Squared Error) between the predicted ratings and the actual ratings. The ALS model is trained using the fit() method with the training data.\n",
    "\n",
    "The trained model is used to generate predictions on the validation data using the transform() method. The predictions DataFrame is then displayed, showing the first 10 rows."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4cc2cee8294b3c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(ratings)\n",
    "    for column in [\"user_id\", \"product_id\"]\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "ratings_indexed = pipeline.fit(ratings).transform(ratings)\n",
    "\n",
    "training_data,validation_data = ratings_indexed.randomSplit([8.0,2.0])\n",
    "\n",
    "als = ALS(userCol=\"user_id_index\",itemCol=\"product_id_index\",ratingCol=\"score\",rank=10,maxIter=5,regParam=0.01,coldStartStrategy=\"drop\")\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",labelCol=\"score\",predictionCol=\"prediction\")\n",
    "\n",
    "model = als.fit(training_data)\n",
    "predictions=model.transform(validation_data)\n",
    "predictions.show(10,False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e95efda43b17e697"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ALS algorithm for collaborative filtering\n",
    "\n",
    "Collaborative filtering is a popular technique used in recommendation systems to generate personalized recommendations for users based on their interactions with items (e.g., products, movies, articles) and similarities with other users. It operates on the principle of leveraging the collective wisdom of users to make predictions about preferences. There are two main types of collaborative filtering:\n",
    "\n",
    "1. **User-based Collaborative Filtering:** This approach recommends items to a user based on the preferences of similar users. It identifies users with similar behaviors or tastes and suggests items that they have liked or interacted with.\n",
    "\n",
    "2. **Item-based Collaborative Filtering:** In this method, recommendations are made by identifying similar items to the ones a user has interacted with. It looks for items that have been liked or rated similarly by other users and suggests them to the target user.\n",
    "\n",
    "Collaborative filtering does not require explicit knowledge of item characteristics or user preferences but relies on observed interactions between users and items. It is widely used in e-commerce platforms, streaming services, and content recommendation systems to enhance user experience and engagement.\n",
    "\n",
    "\n",
    "We first filter the `validation_data` DataFrame to select data related to a specific user with `user_id_index` equal to 1.0. Then, we apply the ALS model to generate recommendations for this user based on their interactions with products. The resulting recommendations are ordered by prediction score in descending order and displayed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10ea6e9ba6ecfcac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user1 = validation_data.filter(validation_data['user_id_index']==1.0).select(['product_id','user_id','user_id_index','product_id_index'])\n",
    "user1.show()\n",
    "recommendations = model.transform(user1) \n",
    "recommendations.orderBy('prediction',ascending=False).show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f92c38321389d96e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the performance of the recommendation model using RMSE and MAE\n",
    "\n",
    "We calculate the Root Mean Squared Error (RMSE) to evaluate the model's performance in predicting user ratings. The RMSE measures the differences between predicted and actual ratings, providing an overall assessment of model accuracy. Additionally, we compute the Mean Absolute Error (MAE) as an alternative evaluation metric, which represents the average absolute difference between predicted and actual ratings.\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "RMSE = sqrt(Σ(predicted_rating - actual_rating)^2 / n)\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "MAE = Σ|predicted_rating - actual_rating| / n\n",
    "\n",
    "Here:\n",
    "- predicted_rating: the predicted rating by the recommendation model.\n",
    "- actual_rating: the actual rating provided by the user.\n",
    "- n: the total number of predictions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2c01db661a1fe80"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) = {rmse}\")\n",
    "\n",
    "# Additional Evaluation Metric: Mean Absolute Error (MAE)\n",
    "evaluator_mae = RegressionEvaluator(\n",
    "    metricName=\"mae\",\n",
    "    labelCol=\"score\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(f\"Mean Absolute Error (MAE) = {mae}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68530fdd30371ea1"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "856466dda7587933"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Monte Carlo Simulation\n",
    "\n",
    "Financial stock risk analysis involves evaluating the potential uncertainties and volatility associated with investing in a particular stock or the overall market. Investors and analysts use various tools and methodologies to assess factors that may impact stock prices, such as market trends, economic indicators, company performance, and geopolitical events. Quantitative measures, including standard deviation, beta, and value at risk (VaR), are often employed to quantify and analyze the level of risk in a stock or portfolio. Fundamental analysis examines a company's financial health, earnings, and growth potential, while technical analysis studies historical price and trading volume patterns. Additionally, sentiment analysis considers market sentiment and news to gauge investor behavior. By conducting comprehensive risk analysis, investors can make more informed decisions, develop risk management strategies, and optimize their investment portfolios in response to changing market conditions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a5e818d0fcaefc1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"4g\").appName('MCS').getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0b1b6b77fcad8ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading and Preparation\n",
    "\n",
    "We start by loading the stock and factor data into Spark DataFrames. The `stocks` DataFrame contains information about various stocks, while the `factors` DataFrame contains additional factors that may influence stock prices. Both DataFrames are read from CSV files located in the specified directories.\n",
    "\n",
    "We then perform some preprocessing steps on the `stocks` and `factors` DataFrames, including extracting the stock symbols from file paths, filtering the data based on date ranges, and converting date formats.\n",
    "\n",
    "After preparing the data, we convert the Spark DataFrames to Pandas DataFrames for further analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4f8856bd70f6625"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import Window\n",
    "from datetime import datetime\n",
    "\n",
    "stocks = spark.read.csv([\"data/stocksA/ABAX.csv\",\"data/stocksA/AAME.csv\",\"data/stocksA/AEPI.csv\"], header='true', inferSchema='true')\n",
    "\n",
    "stocks = stocks.withColumn(\"Symbol\", fun.input_file_name()).withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"/\"), -1))\\\n",
    "                .withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))\n",
    "stocks.show(2)\n",
    "factors = spark.read.csv([\"data/stocks/ABAX.csv\",\"data/stocks/AAME.csv\",\"data/stocks/AEPI.csv\"], header='true', inferSchema='true')\n",
    "factors = factors.withColumn(\"Symbol\", fun.input_file_name()).withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"/\"), -1)).\\\n",
    "                    withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))\n",
    "\n",
    "stocks = stocks.withColumn('count', fun.count('Symbol').over(Window.partitionBy('Symbol'))).filter(fun.col('count') > 260*5 + 10)\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "stocks = stocks.withColumn('Date',fun.to_date(fun.to_timestamp(fun.col('Date'),'dd-MMM-yy')))\n",
    "stocks.printSchema()\n",
    "\n",
    "stocks = stocks.filter(fun.col('Date') >= datetime(2009, 10, 23)).filter(fun.col('Date') <= datetime(2014, 10, 23))\n",
    "factors = factors.withColumn('Date',fun.to_date(fun.to_timestamp(fun.col('Date'),'dd-MMM-yy')))\n",
    "factors = factors.filter(fun.col('Date') >= datetime(2009, 10, 23)).filter(fun.col('Date') <= datetime(2014, 10, 23))\n",
    "factors.show(2)\n",
    "\n",
    "stocks_pd_df = stocks.toPandas()\n",
    "factors_pd_df = factors.toPandas()\n",
    "factors_pd_df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be675c424d062046"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Determining the Factor Weights\n",
    "\n",
    "Here, we determine the factor weights for each stock using linear regression. We calculate the returns of stocks and factors over a rolling window of 10 steps, then apply linear regression to estimate the coefficients of factors contributing to stock returns.\n",
    "\n",
    "\n",
    "1. **Defining Variables and Functions:**\n",
    "      - Here, we define variables and a custom function necessary for our calculations.\n",
    "\n",
    "\n",
    "2. **Calculating Rolling Returns:**\n",
    "      - Rolling returns for both stocks and factors are calculated using a rolling window approach.\n",
    "\n",
    "\n",
    "3. **Combining Returns with DataFrames:**\n",
    "      - Rolling returns are combined with their respective DataFrames for further analysis.\n",
    "\n",
    "\n",
    "4. **Merging DataFrames and Preprocessing:**\n",
    "      - The DataFrames are merged, and preprocessing steps, such as handling missing values, are performed.\n",
    "\n",
    "\n",
    "5. **Linear Regression and Coefficients Calculation:**\n",
    "     - Finally, linear regression is performed, and coefficients are calculated for each stock.\n",
    "\n",
    "\n",
    "Renames columns and organizes the DataFrame for clarity and readability.\n",
    "Returns the DataFrame containing OLS coefficients per stock.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a05b69c38fa62c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "n_steps = 10\n",
    "def my_fun(x):\n",
    "    return ((x.iloc[-1] - x.iloc[0]) / x.iloc[0])\n",
    "\n",
    "stock_returns = stocks_pd_df.groupby('Symbol').Close.rolling(window=n_steps).apply(my_fun)\n",
    "factors_returns = factors_pd_df.groupby('Symbol').Close.rolling(window=n_steps).apply(my_fun)\n",
    "stock_returns = stock_returns.reset_index().sort_values('level_1').reset_index()\n",
    "factors_returns = factors_returns.reset_index().sort_values('level_1').reset_index()\n",
    "\n",
    "stocks_pd_df_with_returns = stocks_pd_df.assign(stock_returns = stock_returns['Close'])\n",
    "factors_pd_df_with_returns = factors_pd_df.assign(factors_returns = \\\n",
    "factors_returns['Close'],factors_returns_squared = factors_returns['Close']**2)\n",
    "factors_pd_df_with_returns = factors_pd_df_with_returns.pivot(index='Date',columns='Symbol',values=['factors_returns', 'factors_returns_squared'])\n",
    "factors_pd_df_with_returns.columns = factors_pd_df_with_returns.columns.to_series().str.join('_').reset_index()[0]\n",
    "factors_pd_df_with_returns = factors_pd_df_with_returns.reset_index()\n",
    "print(factors_pd_df_with_returns.head(1))\n",
    "print(factors_pd_df_with_returns.columns)\n",
    "\n",
    "# For each stock, create input DF for linear regression training\n",
    "stocks_factors_combined_df = pd.merge(stocks_pd_df_with_returns,factors_pd_df_with_returns,how=\"left\", on=\"Date\")\n",
    "feature_columns = list(stocks_factors_combined_df.columns[-6:])\n",
    "\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    stocks_factors_combined_df = stocks_factors_combined_df.dropna(subset=feature_columns + ['stock_returns'])\n",
    "\n",
    "def find_ols_coef(df):\n",
    "    y = df[['stock_returns']].values\n",
    "    X = df[feature_columns]\n",
    "    regr = LinearRegression()\n",
    "    regr_output = regr.fit(X, y)\n",
    "    return list(df[['Symbol']].values[0]) + list(regr_output.coef_[0])\n",
    "\n",
    "coefs_per_stock = stocks_factors_combined_df.groupby('Symbol').apply(find_ols_coef)\n",
    "coefs_per_stock = pd.DataFrame(coefs_per_stock).reset_index()\n",
    "coefs_per_stock.columns = ['symbol', 'factor_coef_list']\n",
    "coefs_per_stock = pd.DataFrame(coefs_per_stock.factor_coef_list.tolist(),index=coefs_per_stock.index,columns = ['Symbol'] + feature_columns)\n",
    "coefs_per_stock"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93a5e825952d1d01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sampling\n",
    "\n",
    "In this section, we sample from factor returns data to analyze return distributions for each factor. Kernel Density Estimation (KDE) plots visualize return distributions for each factor, and we calculate the correlation matrix between sampled factors."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e50735c8aa4c38fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "\n",
    "samples = factors_returns.loc[factors_returns.Symbol == \\\n",
    "factors_returns.Symbol.unique()[0]]['Close']\n",
    "samples.plot.kde()\n",
    "\n",
    "f_1 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[0]]['Close']\n",
    "f_2 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[1]]['Close']\n",
    "f_3 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[2]]['Close']\n",
    "print(f_1.size,len(f_2),f_3.size)\n",
    "pd.DataFrame({'f1': list(f_1)[1:1040], 'f2': list(f_2)[1:1040], 'f3':list(f_3)}).corr()\n",
    "\n",
    "factors_returns_cov = pd.DataFrame({'f1': list(f_1)[1:1040],'f2': list(f_2)[1:1040],'f3': list(f_3)}).cov().to_numpy()\n",
    "factors_returns_mean = pd.DataFrame({'f1': list(f_1)[1:1040],'f2': list(f_2)[1:1040],'f3': list(f_3)}).mean()\n",
    "\n",
    "multivariate_normal(factors_returns_mean, factors_returns_cov)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8955f1ae705e1c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running the Trials\n",
    "\n",
    "Finally, we conduct Monte Carlo simulations to generate trial returns for each stock. We use a multivariate normal  distribution to generate random factors based on the mean and covariance of factor returns. Subsequently, we apply coefficients obtained from linear regression to calculate expected returns for each stock under different scenarios. The trials are executed in parallel across multiple threads for efficiency.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2edfbdecb77afc70"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "from numpy.random import seed\n",
    "from pyspark.sql.types import LongType, ArrayType, DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "b_coefs_per_stock = spark.sparkContext.broadcast(coefs_per_stock)\n",
    "b_feature_columns = spark.sparkContext.broadcast(feature_columns)\n",
    "b_factors_returns_mean = spark.sparkContext.broadcast(factors_returns_mean)\n",
    "b_factors_returns_cov = spark.sparkContext.broadcast(factors_returns_cov)\n",
    "\n",
    "parallelism = 1000\n",
    "num_trials = 1000000\n",
    "base_seed = 1496\n",
    "seeds = [b for b in range(base_seed,\n",
    "base_seed + parallelism)]\n",
    "seedsDF = spark.createDataFrame(seeds, IntegerType())\n",
    "seedsDF = seedsDF.repartition(parallelism)\n",
    "\n",
    "def calculate_trial_return(x):\n",
    "    trial_return_list = []\n",
    "    for i in range(int(num_trials/parallelism)):\n",
    "        random_int = random.randint(0, num_trials*num_trials)\n",
    "        seed(x)\n",
    "        random_factors = multivariate_normal(b_factors_returns_mean.value,\n",
    "        b_factors_returns_cov.value)\n",
    "        coefs_per_stock_df = b_coefs_per_stock.value\n",
    "        returns_per_stock = (coefs_per_stock_df[b_feature_columns.value] *(list(random_factors) + list(random_factors**2)))\n",
    "        trial_return_list.append(float(returns_per_stock.sum(axis=1).sum(), b_coefs_per_stock.value.size))\n",
    "    return trial_return_list\n",
    "\n",
    "udf_return = udf(calculate_trial_return, ArrayType(DoubleType()))\n",
    "\n",
    "def calculate_trial_return(x):\n",
    "    trial_return_list = []\n",
    "    for i in range(int(num_trials / parallelism)):\n",
    "        random_int = random.randint(0, num_trials * num_trials)\n",
    "        seed(x)\n",
    "        random_factors = multivariate_normal(b_factors_returns_mean.value, b_factors_returns_cov.value)\n",
    "        coefs_per_stock_df = b_coefs_per_stock.value\n",
    "        returns_per_stock = (coefs_per_stock_df[b_feature_columns.value] * (list(random_factors) + list(random_factors**2)))\n",
    "        trial_return_list.append(float(returns_per_stock.sum(axis=1).sum()) / b_coefs_per_stock.value.size)\n",
    "        trial_return_list.append(float(returns_per_stock.sum(axis=1).sum()) / b_coefs_per_stock.value.size)\n",
    "    return trial_return_list\n",
    "udf_return = udf(calculate_trial_return, ArrayType(DoubleType()))\n",
    "\n",
    "trials = seedsDF.withColumn(\"trial_return\", udf_return(col(\"value\")))\n",
    "trials = trials.select('value', explode('trial_return').alias('trial_return'))\n",
    "trials.cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5efc268b6cf0bf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of Trial Returns\n",
    "\n",
    "In this section, we evaluate the trial returns generated from Monte Carlo simulations. We calculate the 5th percentile of trial returns as a measure of downside risk and compute the average trial return for further analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ac7e76f0c073408"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of Trial Returns\n",
    "\n",
    "In this section, we evaluate the trial returns generated from Monte Carlo simulations. We calculate the 5th percentile of trial returns as a measure of downside risk and compute the average trial return for further analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c0e2825ac06dd2c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trials.approxQuantile('trial_return', [0.05], 0.0)\n",
    "\n",
    "trials.orderBy(col('trial_return').asc()).limit(int(trials.count()/20)).agg(fun.avg(col(\"trial_return\"))).show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94e60dc54d3715a0"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a074111ee2c7f016"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
